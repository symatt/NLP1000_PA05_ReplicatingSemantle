{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3i3m9JjeM5U5"
   },
   "source": [
    "# **Programming Assessment \\#5**\n",
    "\n",
    "Names: Go, Wilfred | Sibug, Jordan | Sy, James Matthew\n",
    "\n",
    "More information on the assessment is found in our Canvas course. Link: https://dlsu.instructure.com/courses/93383/assignments/841058/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "HxtmCAZwNoeU"
   },
   "source": [
    "# **Load Pre-trained Embeddings**\n",
    "\n",
    "*While you don't have to separate your code into blocks, it might be easier if you separated loading / downloading your data from the main part of your solution. Consider placing all loading of data into the code block below.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "CbvxU2oTM4IV"
   },
   "outputs": [],
   "source": [
    "# NOTE: need to only run once to create the word2vec file from GLOVE txt file.\n",
    "from gensim.scripts.glove2word2vec import glove2word2vec\n",
    "# https://nlp.stanford.edu/projects/glove/\n",
    "# download the glove.6B.zip and use the glove.6B.200d.txt file\n",
    "glove_input_file = 'glove.6B.200d.txt'\n",
    "word2vec_output_file = 'glove.6B.200d.txt.word2vec'\n",
    "glove2word2vec(glove_input_file, word2vec_output_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models import KeyedVectors\n",
    "# load the Stanford GloVe model\n",
    "filename = 'glove.6B.200d.txt.word2vec'\n",
    "model = KeyedVectors.load_word2vec_format(filename, binary=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import regex as re\n",
    "import numpy as np\n",
    "from gensim.models import Word2Vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# cosine similarity\n",
    "def getCosineSimilarity(targetWord, inputWord):\n",
    "    return model.distance(targetWord, inputWord)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "r8YCZLi-N0uR"
   },
   "source": [
    "# **Your Implementation**\n",
    "\n",
    "*Again, you don't have to have everything in one block. Use the notebook according to your preferences with the goal of fulfilling the assessment in mind.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "VqKjpUrkOSnC"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Randomly selected word: gepid\n",
      "\n",
      "Closest...\n",
      "10th: airg√≠alla 0.5438644\n",
      "100th: chichimec 0.4824953\n",
      "1000th: sirdal 0.41408923\n",
      "\n",
      "Your guess: gepid\n",
      "word: 1.0\n",
      "Great job on guessing the word!\n"
     ]
    }
   ],
   "source": [
    "pattern = r'([\\@\\.\\'\\\"\\-\\_\\+\\:\\s0-9])+'\n",
    "randomIndex = random.randrange(0, len(model))\n",
    "randomWord = model.index_to_key[randomIndex]\n",
    "# print('Randomly selected word: ' + randomWord)\n",
    "notWord = re.search(pattern, randomWord)\n",
    "# print(notWord)\n",
    "while notWord:\n",
    "    randomIndex = random.randrange(0, len(model))\n",
    "    randomWord = model.index_to_key[randomIndex]\n",
    "#     print('Randomly selected word: ' + randomWord)\n",
    "    notWord = re.search(pattern, randomWord)\n",
    "#     print(notWord)\n",
    "# get all cosine similarities\n",
    "# -1 to +1 (opposite meaning for -1)\n",
    "similarities = model.cosine_similarities(model.vectors[randomIndex], model.vectors[:])\n",
    "sims = []\n",
    "for i in range(len(similarities)):\n",
    "    sims.append([model.index_to_key[i], similarities[i]])\n",
    "sims.sort(key=lambda x: x[1], reverse=True)\n",
    "print('Randomly selected word: ' + randomWord)\n",
    "print()\n",
    "print('Closest...')\n",
    "print('10th: ' + str(sims[9][0]) + ' ' + str(sims[9][1]))\n",
    "print('100th: ' + str(sims[99][0]) + ' ' + str(sims[99][1]))\n",
    "print('1000th: ' + str(sims[999][0]) + ' ' + str(sims[999][1]))\n",
    "print()\n",
    "# ask user for word\n",
    "guessed = False\n",
    "while (not guessed):\n",
    "    inputWord = input('Your guess: ').lower()\n",
    "    # get the similarity score\n",
    "    try :\n",
    "        model.get_vector(inputWord)\n",
    "        print('word: ' + str(model.similarity(randomWord, inputWord)))\n",
    "    except:\n",
    "        print('Word not recognized in resource.')\n",
    "    # print score\n",
    "    if inputWord == randomWord:\n",
    "        print('Great job on guessing the word!')\n",
    "        guessed = True\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "h3smvUR6OXUa"
   },
   "source": [
    "# **Your Relfection / Takeaway / Analysis**\n",
    "\n",
    "*Kindly place the rest of your write up. More information is found in the Canvas write up.*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Analysis\n",
    "\n",
    "We took a straight forward approach in implementing the program. After looking into the data set and familiarizing ourselves with the datatypes and features that are going to work with, we started reading up on the gensim library and which pre-made functions we can use for the program. The program starts off by getting a random word from the vocabulary we considered the difficulty of the program when the winning word to be guessed had punctuations in it or numbers and settled on the decision that the winning word should only consist of letters. Since semantle only takes in words, we wanted to have a similar feel while playing our own application. However, the GloVe dataset was not cleaned to remove the words that have punctuations and numbers and only the winning word was taken into account. Therefore, there may be instances where the hint words are numbers or words with punctuations. It then gets the cosine similarity of the winning word to all the words in the vocabulary, the variable holding the similarities of the word to all the words is then appended along with each compared word to an array with the first dimension being the word and the second dimension containing the cosine similarity of the word to the winning word. The array is then sorted according to the cosine similarity in a descending order and the program prints the 9th, 99th, and 999th index to show the 10th, 100th, and 1000th closest word to the winning word according to cosine similarity. Finally, the program asks the user for an input and this input is checked for its cosine similarity to the winning word. The program ends when the winning word is guessed by the user."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Takeaways\n",
    "\n",
    "We tried to use to 50D and the 200D Glove dataset to test the results of the cosine similarities of the words. With 50D, the words closest to 'so' were 'be'(10th), 'basically'(100th) and 'year'(1000th). On the other hand, the 200D showed 'n't'(10th), 'few'(100th) and 'closely'(1000th) as the closest words to 'so'. After observing the program work on two different pre-trained word embeddings the dimensions of the pre-trained embedding had an effect on the results of the cosine similarities. It could be that since the dimensions of the embedding are quite far apart and the 200 dimension embeddings had more prior knowledge that resulted into a different answer. It is good to note that while increasing the dimension may increase the accuracy it comes at the cost of computational complexity."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Reflection\n",
    "\n",
    "What stood out the most for all of us was the ease of use of the gensim functions where almost everything we needed to access the model were already there. The project has motivated us to learn more about how words are embedded and the exact process in which the model learns to relate the similarity and dissimilarity various different words. This experience has reinforced our knowledge about vector semantics and more specifically word similarity by letting us experience the application of it first hand. The implementation of the program is the roughly the same as semantle however it varies slightly, an example of this is when giving the hints the program immediately gives out the 10th, 100th, and 1000th closest word while in semantle it slowly reveals words closer to the word itself and only the top 1000 closest words are taken into account when guessing, while in the program all the words even cold ones show the exact similarity. Additionally, there are certain words that are not just one word that can be an input in Semantle i.e. ice cream, while in the program, it just uses one word for each input."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "name": "PA5_Replicating_Semantle.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
